/**
 * ChatAgent Tests
 *
 * Tests the ReAct-based chat agent that wraps ReActLoop + ConversationContext.
 * Verifies:
 * - Correct event sequence for direct answers (no tool calls)
 * - Correct event sequence for tool-augmented answers
 * - Conversation context management (add, truncate, rollback)
 * - Source collection from tool results
 * - /clear and /focus reconfiguration
 * - Error handling and context rollback
 */

import { describe, it, expect, vi, beforeEach } from 'vitest';
import type { LLMProvider, ChatMessage, StreamChunk, ChatResponse, ToolDefinition, GenerateOptions } from '@contextaisdk/core';
import { ChatAgent, type ChatAgentEvent, type ChatAgentConfig } from '../chat-agent.js';
import type { RoutingRAGEngine } from '../routing-rag-engine.js';
import type { ProjectMetadata } from '../query-router.js';
import type { Project } from '../../database/schema.js';

// ============================================================================
// Test Helpers
// ============================================================================

/**
 * Create a mock LLM provider that yields predetermined stream chunks.
 *
 * The streamChat function returns chunks based on a sequence:
 * Each call pops the next response from the queue.
 */
function createMockLLMProvider(
  responses: Array<{ textChunks: string[]; toolCalls?: Array<{ id: string; name: string; arguments: Record<string, unknown> }> }>
): LLMProvider {
  let callIndex = 0;

  return {
    name: 'test-provider',
    model: 'test-model',
    chat: vi.fn().mockResolvedValue({
      content: 'test response',
      finishReason: 'stop',
    } as ChatResponse),
    isAvailable: vi.fn().mockResolvedValue(true),
    streamChat: vi.fn(async function* (_messages: ChatMessage[], _options?: GenerateOptions) {
      const response = responses[callIndex % responses.length];
      callIndex++;

      // Yield text chunks
      for (const text of response.textChunks) {
        yield { type: 'text' as const, content: text } as StreamChunk;
      }

      // Yield tool call chunks if any
      if (response.toolCalls) {
        for (const tc of response.toolCalls) {
          yield {
            type: 'tool_call' as const,
            toolCall: { id: tc.id, name: tc.name, arguments: JSON.stringify(tc.arguments) },
          } as StreamChunk;
        }
      }

      // Yield usage
      yield { type: 'usage' as const, usage: { totalTokens: 100 } } as StreamChunk;

      // Yield done
      yield { type: 'done' as const } as StreamChunk;
    }),
  };
}

function createMockRoutingEngine(): RoutingRAGEngine {
  return {
    search: vi.fn().mockResolvedValue({
      content: '<sources>test context</sources>',
      estimatedTokens: 50,
      sources: [{
        index: 1,
        filePath: 'src/auth.ts',
        lineRange: { start: 1, end: 10 },
        score: 0.9,
        language: 'typescript',
        fileType: 'code',
      }],
      rawResults: [],
      metadata: {
        retrievalMs: 30,
        assemblyMs: 5,
        totalMs: 35,
        resultsRetrieved: 5,
        resultsAssembled: 1,
        fromCache: false,
      },
      routing: {
        method: 'heuristic',
        projectIds: ['project-1'],
        confidence: 0.9,
        reason: 'test',
      },
    }),
    dispose: vi.fn(),
  } as unknown as RoutingRAGEngine;
}

function createMockProject(): Project {
  return {
    id: 'project-1',
    name: 'my-app',
    path: '/tmp/my-app',
    description: 'Test project',
    tags: '["test"]',
    file_count: 10,
    chunk_count: 50,
    created_at: '2024-01-01',
    updated_at: '2024-01-01',
  } as Project;
}

function createMockProjects(): ProjectMetadata[] {
  return [{
    id: 'project-1',
    name: 'my-app',
    description: 'Test project',
    tags: ['test'],
    fileCount: 10,
    chunkCount: 50,
  }];
}

/**
 * Collect all events from a ChatAgent stream.
 */
async function collectEvents(
  gen: AsyncGenerator<ChatAgentEvent, void, unknown>
): Promise<ChatAgentEvent[]> {
  const events: ChatAgentEvent[] = [];
  for await (const event of gen) {
    events.push(event);
  }
  return events;
}

function createDefaultConfig(overrides: Partial<ChatAgentConfig> = {}): ChatAgentConfig {
  return {
    llmProvider: createMockLLMProvider([{ textChunks: ['Hello world'] }]),
    routingEngine: null,
    allProjects: [],
    currentProject: null,
    ...overrides,
  };
}

// ============================================================================
// Tests
// ============================================================================

describe('ChatAgent', () => {
  describe('construction', () => {
    it('creates a ChatAgent successfully with minimal config', () => {
      const agent = new ChatAgent(createDefaultConfig());
      expect(agent).toBeDefined();
      expect(agent.getContext()).toBeDefined();
    });

    it('creates a ChatAgent with routing engine and projects', () => {
      const agent = new ChatAgent(createDefaultConfig({
        routingEngine: createMockRoutingEngine(),
        allProjects: createMockProjects(),
        currentProject: createMockProject(),
      }));
      expect(agent).toBeDefined();
    });
  });

  describe('streamQuestion - direct answer (no tool calls)', () => {
    it('yields thinking_delta, thinking_complete, and response_complete', async () => {
      const llm = createMockLLMProvider([
        { textChunks: ['Hello', ' world', '!'] },
      ]);

      const agent = new ChatAgent(createDefaultConfig({ llmProvider: llm }));
      const events = await collectEvents(agent.streamQuestion('Hi there'));

      // Should have thinking_delta for each text chunk
      const thinkingDeltas = events.filter(e => e.type === 'thinking_delta');
      expect(thinkingDeltas).toHaveLength(3);
      expect(thinkingDeltas[0]).toEqual({
        type: 'thinking_delta',
        content: 'Hello',
        iteration: 1,
      });

      // Should have thinking_complete
      const thinkingCompletes = events.filter(e => e.type === 'thinking_complete');
      expect(thinkingCompletes).toHaveLength(1);
      expect(thinkingCompletes[0]).toEqual({
        type: 'thinking_complete',
        content: 'Hello world!',
        iteration: 1,
      });

      // Should have response_complete
      const completions = events.filter(e => e.type === 'response_complete');
      expect(completions).toHaveLength(1);
      expect(completions[0]).toMatchObject({
        type: 'response_complete',
        content: 'Hello world!',
        sources: [],
      });
    });

    it('adds messages to conversation context', async () => {
      const llm = createMockLLMProvider([{ textChunks: ['Response'] }]);
      const agent = new ChatAgent(createDefaultConfig({ llmProvider: llm }));

      await collectEvents(agent.streamQuestion('Hello'));

      const messages = agent.getContext().getMessages();
      expect(messages).toHaveLength(2);
      expect(messages[0]).toEqual({ role: 'user', content: 'Hello' });
      expect(messages[1]).toEqual({ role: 'assistant', content: 'Response' });
    });
  });

  describe('streamQuestion - with tool calls', () => {
    it('yields tool events when agent calls retrieve_knowledge', async () => {
      const engine = createMockRoutingEngine();
      const projects = createMockProjects();

      // First call: LLM produces reasoning + tool call
      // Second call: LLM produces final answer after seeing tool result
      const llm = createMockLLMProvider([
        {
          textChunks: ['Let me search'],
          toolCalls: [{
            id: 'call-1',
            name: 'retrieve_knowledge',
            arguments: { query: 'authentication' },
          }],
        },
        {
          textChunks: ['Auth uses JWT tokens'],
        },
      ]);

      const agent = new ChatAgent(createDefaultConfig({
        llmProvider: llm,
        routingEngine: engine,
        allProjects: projects,
        currentProject: createMockProject(),
      }));

      const events = await collectEvents(agent.streamQuestion('How does auth work?'));

      // Should have tool_start event
      const toolStarts = events.filter(e => e.type === 'tool_start');
      expect(toolStarts).toHaveLength(1);
      expect(toolStarts[0]).toMatchObject({
        type: 'tool_start',
        tool: 'retrieve_knowledge',
      });

      // Should have tool_result event
      const toolResults = events.filter(e => e.type === 'tool_result');
      expect(toolResults).toHaveLength(1);
      expect(toolResults[0]).toMatchObject({
        type: 'tool_result',
        tool: 'retrieve_knowledge',
        success: true,
      });

      // Should have response_complete with sources
      const completions = events.filter(e => e.type === 'response_complete');
      expect(completions).toHaveLength(1);
      expect(completions[0]).toMatchObject({
        type: 'response_complete',
        content: 'Auth uses JWT tokens',
      });
      // Sources should be collected from tool result
      const completion = completions[0] as Extract<ChatAgentEvent, { type: 'response_complete' }>;
      expect(completion.sources.length).toBeGreaterThanOrEqual(0);
    });
  });

  describe('clearHistory', () => {
    it('clears conversation context', async () => {
      const llm = createMockLLMProvider([{ textChunks: ['Response'] }]);
      const agent = new ChatAgent(createDefaultConfig({ llmProvider: llm }));

      // Add some history
      await collectEvents(agent.streamQuestion('Hello'));
      expect(agent.getContext().getMessages()).toHaveLength(2);

      // Clear
      agent.clearHistory();
      expect(agent.getContext().getMessages()).toHaveLength(0);
    });
  });

  describe('updateProject', () => {
    it('updates project context for tool closures', async () => {
      const engine1 = createMockRoutingEngine();
      const engine2 = createMockRoutingEngine();
      const projects1 = createMockProjects();
      const projects2: ProjectMetadata[] = [{
        id: 'project-2',
        name: 'other-app',
        description: 'Another project',
        tags: [],
        fileCount: 5,
        chunkCount: 25,
      }];

      const agent = new ChatAgent(createDefaultConfig({
        routingEngine: engine1,
        allProjects: projects1,
        currentProject: createMockProject(),
      }));

      // Switch to a different project
      const newProject = { ...createMockProject(), id: 'project-2', name: 'other-app' };
      agent.updateProject(newProject, projects2, engine2);

      // The agent's context should reflect the change
      // (verified by checking that subsequent operations work)
      expect(agent.getContext()).toBeDefined();
    });
  });

  describe('file reference context', () => {
    it('includes file reference context in messages', async () => {
      const llm = createMockLLMProvider([{ textChunks: ['Response'] }]);
      const agent = new ChatAgent(createDefaultConfig({ llmProvider: llm }));

      await collectEvents(agent.streamQuestion('test', {
        fileReferenceContext: '## src/auth.ts\n```\nfunction auth() {}\n```',
      }));

      // Verify streamChat was called with file reference as system message
      const calls = vi.mocked(llm.streamChat).mock.calls;
      expect(calls).toHaveLength(1);
      const messages = calls[0][0];
      const fileRefMsg = messages.find(m =>
        m.role === 'system' && typeof m.content === 'string' && m.content.includes('File References')
      );
      expect(fileRefMsg).toBeDefined();
    });
  });

  describe('multi-turn conversation', () => {
    it('maintains conversation history across questions', async () => {
      const llm = createMockLLMProvider([
        { textChunks: ['First response'] },
        { textChunks: ['Second response'] },
      ]);

      const agent = new ChatAgent(createDefaultConfig({ llmProvider: llm }));

      // First question
      await collectEvents(agent.streamQuestion('First question'));

      // Second question
      await collectEvents(agent.streamQuestion('Follow-up'));

      // Context should have both exchanges
      const messages = agent.getContext().getMessages();
      expect(messages).toHaveLength(4);
      expect(messages[0]).toEqual({ role: 'user', content: 'First question' });
      expect(messages[1]).toEqual({ role: 'assistant', content: 'First response' });
      expect(messages[2]).toEqual({ role: 'user', content: 'Follow-up' });
      expect(messages[3]).toEqual({ role: 'assistant', content: 'Second response' });
    });
  });

  describe('max iterations graceful degradation', () => {
    it('yields response_complete instead of error when max iterations exceeded', async () => {
      const engine = createMockRoutingEngine();
      const projects = createMockProjects();

      // LLM always calls tools â€” will exhaust maxIterations
      const llm = createMockLLMProvider([
        {
          textChunks: ['Searching for info'],
          toolCalls: [{
            id: 'call-1',
            name: 'retrieve_knowledge',
            arguments: { query: 'test' },
          }],
        },
        {
          textChunks: ['Let me search again'],
          toolCalls: [{
            id: 'call-2',
            name: 'retrieve_knowledge',
            arguments: { query: 'test again' },
          }],
        },
        {
          textChunks: ['Still searching'],
          toolCalls: [{
            id: 'call-3',
            name: 'retrieve_knowledge',
            arguments: { query: 'more test' },
          }],
        },
      ]);

      const agent = new ChatAgent(createDefaultConfig({
        llmProvider: llm,
        routingEngine: engine,
        allProjects: projects,
        currentProject: createMockProject(),
        maxIterations: 3, // Low limit to trigger graceful degradation
      }));

      const events = await collectEvents(agent.streamQuestion('What is this?'));

      // Should NOT have an error event
      const errors = events.filter(e => e.type === 'error');
      expect(errors).toHaveLength(0);

      // Should have a response_complete with the last thought content
      const completions = events.filter(e => e.type === 'response_complete');
      expect(completions).toHaveLength(1);
      expect(completions[0]).toMatchObject({
        type: 'response_complete',
      });

      // Conversation context should be preserved (not rolled back)
      const messages = agent.getContext().getMessages();
      expect(messages.length).toBeGreaterThanOrEqual(2); // user + assistant
    });
  });
});
